# Project Handoff Report: Regional Hospital Capacity Planning

**Date:** December 18, 2025  
**Project Root:** `regional-hospital-capacity-planning`  
**GitHub:** [https://github.com/mrluke269/regional-hospital-capacity-planning]

---

## Project Goal

Build a demand sensing pipeline that correlates external disease signals (ED visits) with hospital capacity data to predict capacity crunches 14 days in advance.

---

## Phase Structure

| Phase | Description | Status |
|-------|-------------|--------|
| 1 | Foundation & Data Exploration | ‚úÖ Complete |
| 2 | Data Ingestion Pipeline | üü° In Progress |
| 3 | Data Modeling & Transformation (dbt) | Not Started |
| 4 | Orchestration (Airflow) | Not Started |
| 5 | Visualization (Power BI) | Not Started |
| 6 | Predictive Analytics (Stretch) | Not Started |

---

## Phase 1 Completed

### Infrastructure

| Component | Details | Status |
|-----------|---------|--------|
| AWS S3 | `hospital-planning-raw-luke` (us-east-2) | ‚úÖ |
| AWS EC2 | Ubuntu, Docker installed | ‚úÖ |
| IAM Role | `snowflake_read_s3` updated for new bucket | ‚úÖ |
| Snowflake Storage Integration | `snowflake_s3` pointing to new bucket | ‚úÖ |
| Snowflake Stage | `HOSPITAL_PLANNING_STAGE` in RAW.HOSPITAL_PLANNING | ‚úÖ |
| Docker/Airflow | Running on EC2, accessible at localhost:8080 | ‚úÖ |
| Git/GitHub | Repo created and synced | ‚úÖ |

---

## Phase 2 In Progress: Data Ingestion Pipeline

### Pipeline Architecture

```
Extract (API) ‚Üí Load (S3) ‚Üí COPY INTO (Snowflake)
```

### Hospital Capacity Pipeline - COMPLETE ‚úÖ

| Component | File | Status |
|-----------|------|--------|
| Extract function | `airflow/src/ingestion/extract_hospital_capacity.py` | ‚úÖ |
| S3 load function | `airflow/src/ingestion/load_capacity_S3.py` | ‚úÖ |
| Snowflake load function | `airflow/src/ingestion/copy_to_snowflake.py` | ‚úÖ |
| Snowflake table | `RAW.HOSPITAL_PLANNING.CAPACITY_REPORTS` | ‚úÖ |
| End-to-end test (2 records) | Local Windows | ‚úÖ |

### ED Visits Pipeline - NOT STARTED

| Component | File | Status |
|-----------|------|--------|
| Extract function | `airflow/src/ingestion/extract_ed_visits.py` | ‚ùå |
| S3 load function | `airflow/src/ingestion/load_ed_visits_S3.py` | ‚ùå |
| Snowflake load function | TBD | ‚ùå |
| Snowflake table | TBD | ‚ùå |

---

## Files Created in Phase 2

### extract_hospital_capacity.py

```python
import requests
import time

base_url = "https://data.cdc.gov/resource/ua7e-t2fy.json"
PAGE_SIZE = 1000
RATE_LIMIT_DELAY = 1

def extract_hospital_capacity(max_records=None):
    """
    Extract hospital capacity data from CDC API.
    - max_records=None: fetch all records (production)
    - max_records=N: fetch N records (testing)
    """
    results_combined = []
    count_response = requests.get(base_url, params={"$select": "count(*)"})
    total_count = int(count_response.json()[0]['count'])
    print(f"Total records to fetch: {total_count}")
    
    offset = 0
    
    if max_records is None:
        while offset < total_count:
            response = requests.get(base_url, 
                                params={"$limit": PAGE_SIZE, "$offset": offset})
            data = response.json()
            results_combined.extend(data)
            print(f"Fetched {len(data)} records starting from {offset}")
            offset += len(data)
            time.sleep(RATE_LIMIT_DELAY)
    else:
        response = requests.get(base_url, params={"$limit": max_records})
        data = response.json()
        results_combined.extend(data)
        print(f"Extracted {len(data)} records for testing")
        
    print(f"Total records extracted: {len(results_combined)}")
    return results_combined

if __name__ == "__main__":
    data = extract_hospital_capacity(max_records=2)
    print(f"First record: {data[0]}")
```

### load_capacity_S3.py

```python
import boto3
import json
from datetime import datetime

s3_client = boto3.client('s3')
bucket_name = "hospital-planning-raw-luke"

def load_capacity_S3(data):
    """Upload extracted data to S3 with date-stamped filename."""
    today = datetime.now().strftime("%Y-%m-%d")
    key = f"hospital_capacity_{today}.json"
    
    s3_client.put_object(
        Bucket=bucket_name,
        Key=key,
        Body=json.dumps(data),
    )
    print(f"Data uploaded to s3://{bucket_name}/{key}")
    return key

if __name__ == "__main__":
    import extract_hospital_capacity
    data = extract_hospital_capacity.extract_hospital_capacity(max_records=2)
    load_capacity_S3(data)
    print("Upload complete.")
```

### copy_to_snowflake.py

```python
from config import SNOWFLAKE_CONFIG
import snowflake.connector

def copy_data_to_snowflake(key):
    """Load data from S3 stage into Snowflake table."""
    conn = snowflake.connector.connect(**SNOWFLAKE_CONFIG)
    cs = conn.cursor()

    # Delete existing records for idempotency
    try:
        delete_command = f"""
        DELETE FROM RAW.HOSPITAL_PLANNING.CAPACITY_REPORTS
        WHERE source_file = '{key}';
        """
        cs.execute(delete_command)
    except Exception as e:
        print(f"Error during deletion for idempotency: {e}")
        raise

    # COPY INTO command
    try:
        copy_command = f"""
        COPY INTO RAW.HOSPITAL_PLANNING.CAPACITY_REPORTS(
        raw_data,
        loaded_at,
        source_file
        )
        FROM (
            SELECT
                $1,
                current_timestamp(),
                METADATA$FILENAME
            FROM @RAW.HOSPITAL_PLANNING.HOSPITAL_PLANNING_STAGE/{key}
        )
        FILE_FORMAT = (TYPE = 'JSON', STRIP_OUTER_ARRAY = TRUE)
        FORCE = TRUE;"""

        cs.execute(copy_command)
        result = cs.fetchone()
        print(result)
        
    except Exception as e:
        print(f"Error during COPY INTO operation: {e}")
        raise
    finally:
        cs.close()
        conn.close()

if __name__ == "__main__":
    import extract_hospital_capacity
    data = extract_hospital_capacity.extract_hospital_capacity(max_records=2)
    import load_capacity_S3
    key = load_capacity_S3.load_capacity_S3(data)
    copy_data_to_snowflake(key)
```

### config.py (updated)

```python
import platform

if platform.system() == "Windows":
    KEY_PATH = r"C:\Users\Admin\.ssh\snowflake_key.p8"
else:
    KEY_PATH = "/opt/airflow/keys/snowflake_key.p8"

# Read the private key
with open(KEY_PATH, "rb") as key_file:
    private_key = key_file.read()

SNOWFLAKE_CONFIG = {
    "user": "your_user",
    "account": "your_account",
    "private_key": private_key,
    "warehouse": "your_warehouse",
    "database": "RAW",
    "schema": "HOSPITAL_PLANNING"
}
```

### Snowflake Table DDL

```sql
USE DATABASE RAW;
USE SCHEMA HOSPITAL_PLANNING;

CREATE TABLE CAPACITY_REPORTS (
    raw_data VARIANT,
    loaded_at TIMESTAMP,
    source_file VARCHAR
);
```

---

## Key Technical Decisions

| Decision | Rationale |
|----------|-----------|
| Full load first, incremental later | Prove pipeline works before adding complexity |
| VARIANT column for raw layer | Schema-agnostic, flexible for 190 columns |
| STRIP_OUTER_ARRAY = TRUE | Splits JSON array into individual rows |
| FORCE = TRUE | Allows reloading same file during testing/reprocessing |
| DELETE before COPY INTO | Idempotency - rerunning same day replaces data |
| Date-stamped filenames | Track when each extraction occurred |
| max_records parameter | Easy testing without full 18,693 record load |
| Platform detection for config | Same code works on Windows and Docker/Linux |

---

## Data Sources

### Source 1: Hospital Capacity (Supply) - Pipeline Complete ‚úÖ

- **Endpoint:** `https://data.cdc.gov/resource/ua7e-t2fy.json`
- **Total records:** 18,693
- **Grain:** Weekly, State-level
- **Geography format:** State abbreviations (`AL`, `CA`)
- **API type:** SODA (supports $select, $limit, $offset, $where)

**Selected columns (12):**
- `weekendingdate`, `jurisdiction`
- `numinptbeds`, `numicubeds`
- `numinptbedsocc`, `numicubedsocc`
- `pctinptbedsocc`, `pcticubedsocc`
- `totalconfc19newadm`, `totalconfflunewadm`, `totalconfrsvnewadm`

### Source 2: ED Visits (Demand Signal) - Pipeline Not Started

- **Endpoint:** `https://data.cdc.gov/resource/vjzj-u7u8.json`
- **Grain:** Daily, State-level
- **Geography format:** State full names (`Alabama`, `California`)
- **Pathogens:** COVID, Influenza, RSV, ARI

**Columns (4):**
- `date`, `geography`, `pathogen`, `percent_visits`

---

## Key Integration Challenges

| Issue | Decision |
|-------|----------|
| Geography mismatch | Create state mapping table (abbreviation ‚Üî full name) in dbt |
| Grain mismatch | Aggregate demand from daily ‚Üí weekly in dbt |
| Date overlap | Use Oct 2024 ‚Äì Sep 2025 (~11 months) |

---

## Folder Structure (Updated)

```
regional-hospital-capacity-planning/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îú‚îÄ‚îÄ keys/
‚îÇ   ‚îî‚îÄ‚îÄ snowflake_key.p8          # (gitignored)
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py             # (gitignored)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ingestion/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ extract_hospital_capacity.py   # ‚úÖ NEW
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ load_capacity_S3.py            # ‚úÖ NEW
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ copy_to_snowflake.py           # ‚úÖ NEW
‚îÇ   ‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îî‚îÄ‚îÄ plugins/
‚îÇ       ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ dbt/
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ data_sources.md
‚îÇ   ‚îî‚îÄ‚îÄ infrastructure_setup.md
‚îî‚îÄ‚îÄ notebooks/
    ‚îî‚îÄ‚îÄ 01_data_exploration.ipynb
```

---

## Dependencies

**requirements.txt:**
```
requests
boto3
snowflake-connector-python
```

---

## Next Steps

### Immediate (Phase 2 completion)

1. ‚¨ú Run full load (remove max_records limit) to load all 18,693 records
2. ‚¨ú Build ED visits pipeline (extract ‚Üí S3 ‚Üí Snowflake) - replicate pattern
3. ‚¨ú Create Airflow DAG for hospital capacity pipeline
4. ‚¨ú Test DAG on EC2/Docker

### Future (Phase 3+)

5. ‚¨ú Add incremental loading logic (query Snowflake for max date)
6. ‚¨ú dbt models: staging ‚Üí intermediate ‚Üí marts
7. ‚¨ú State mapping table for geography resolution
8. ‚¨ú Power BI dashboard
9. ‚¨ú Predictive model (stretch)

---

## Reference: Key Paths

| Location | Path |
|----------|------|
| EC2 project root | `~/regional-hospital-capacity-planning` |
| S3 bucket | `s3://hospital-planning-raw-luke/` |
| Snowflake database | `RAW.HOSPITAL_PLANNING` |
| Snowflake stage | `@HOSPITAL_PLANNING_STAGE` |
| Snowflake table | `CAPACITY_REPORTS` |
| Container dags | `/opt/airflow/dags` |
| Container src | `/opt/airflow/src` |
| Container keys | `/opt/airflow/keys` |

---

## Deployment Workflow

### Code Changes
```bash
# Local
git add .
git commit -m "message"
git push origin main

# EC2
cd ~/regional-hospital-capacity-planning
git pull
```

### Secrets (one-time setup)
```powershell
# From local Windows machine
scp -i "C:\Users\Admin\.ssh\EC2_luke.pem" ./keys/snowflake_key.p8 ubuntu@<EC2-IP>:~/regional-hospital-capacity-planning/keys/
scp -i "C:\Users\Admin\.ssh\EC2_luke.pem" ./airflow/src/config.py ubuntu@<EC2-IP>:~/regional-hospital-capacity-planning/airflow/src/
```

### Docker Commands (EC2)
```bash
docker-compose up -d
docker-compose down
docker logs regional-hospital-capacity-planning_airflow-webserver_1 --tail 50
```

### Access Airflow UI
```powershell
ssh -i "C:\Users\Admin\.ssh\EC2_luke.pem" -L 8080:localhost:8080 ubuntu@<EC2-IP>
# Browser: http://localhost:8080 (airflow/airflow)
```

---

## Testing Commands

### Local (Windows)
```powershell
# Test extraction only
python airflow\src\ingestion\extract_hospital_capacity.py

# Test full pipeline (2 records)
python airflow\src\ingestion\copy_to_snowflake.py
```

### Verify in Snowflake
```sql
SELECT COUNT(*) FROM RAW.HOSPITAL_PLANNING.CAPACITY_REPORTS;
SELECT * FROM RAW.HOSPITAL_PLANNING.CAPACITY_REPORTS LIMIT 5;
```